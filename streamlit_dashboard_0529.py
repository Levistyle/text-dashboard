# Streamlit ë””ì‹œì¸ì‚¬ì´ë“œ í…ìŠ¤íŠ¸ ë¶„ì„ ëŒ€ì‹œë³´ë“œ
# âœ… 2025-05-29 ê¸°ì¤€ ì—…ê·¸ë ˆì´ë“œ ë‚´ì—­ ìš”ì•½
# - ìµœì‹  openai.chat.completions.create API ë°©ì‹ ì ìš©
# - API í‚¤ë¥¼ ì½”ë“œ ë‚´ ì§ì ‘ ì…ë ¥ ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •
# - ì›Œë“œí´ë¼ìš°ë“œìš© í°íŠ¸ë¥¼ Windows í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •
# - GPT í•´ì„ ì˜¤ë¥˜ ì²˜ë¦¬ ì™„ë£Œ (message.content ì ‘ê·¼)
# - ì‚¬ìš©ìê°€ ì§ì ‘ 'ë“±ë¡ì–´', 'ë¶ˆìš©ì–´' ì¶”ê°€ ê°€ëŠ¥
# - ë“±ë¡ í›„ ì „ì²´ ë¶„ì„ ê²°ê³¼ ì¬ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ íë¦„ ê°œì„ 
# - ë“±ë¡ëœ í‚¤ì›Œë“œëŠ” txt íŒŒì¼ì— ìë™ ì €ì¥ë¨
# - PDF ë³´ê³ ì„œ í•œê¸€ ì¸ì½”ë”© ì˜¤ë¥˜ í•´ê²°: í•œê¸€ í°íŠ¸ ë“±ë¡

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
from wordcloud import WordCloud
from konlpy.tag import Okt
from collections import Counter
from itertools import combinations
from sklearn.feature_extraction.text import CountVectorizer
from gensim import corpora, models
import openai
import os
from fpdf import FPDF

# GPT API Key ì„¤ì •
import os
openai.api_key = os.getenv("OPENAI_API_KEY")

# ë¶ˆìš©ì–´ ë° ë³´ì¡´ í‚¤ì›Œë“œ íŒŒì¼ ê²½ë¡œ (ë°”íƒ•í™”ë©´)
stopwords_path = os.path.expanduser("~/Desktop/stopwords.txt")
preserve_path = os.path.expanduser("~/Desktop/preserve_keywords.txt")

def load_keywords():
    with open(stopwords_path, encoding="utf-8") as f:
        stopwords = set(line.strip() for line in f)
    with open(preserve_path, encoding="utf-8") as f:
        preserve_keywords = set(line.strip() for line in f)
    return stopwords, preserve_keywords

stopwords, preserve_keywords = load_keywords()

st.title("ğŸ§  ë””ì‹œì¸ì‚¬ì´ë“œ í…ìŠ¤íŠ¸ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

uploaded_file = st.file_uploader("ğŸ“‚ CSV íŒŒì¼ ì—…ë¡œë“œ (title ì»¬ëŸ¼ í¬í•¨)", type=["csv"])

st.sidebar.title("ğŸ”§ í‚¤ì›Œë“œ ê´€ë¦¬")
add_stop = st.sidebar.text_input("ë¶ˆìš©ì–´ ì¶”ê°€")
add_preserve = st.sidebar.text_input("ë“±ë¡ì–´ ì¶”ê°€")
update_keywords = st.sidebar.button("ğŸ“Œ í‚¤ì›Œë“œ ì—…ë°ì´íŠ¸ ë° ë¶„ì„ ì¬ì‹¤í–‰")

if update_keywords:
    if add_stop:
        with open(stopwords_path, "a", encoding="utf-8") as f:
            f.write(add_stop.strip() + "\n")
    if add_preserve:
        with open(preserve_path, "a", encoding="utf-8") as f:
            f.write(add_preserve.strip() + "\n")
    st.sidebar.success("í‚¤ì›Œë“œê°€ ì €ì¥ë˜ì—ˆìœ¼ë©° ë¶„ì„ì´ ì¬ì‹¤í–‰ë©ë‹ˆë‹¤. ë‹¤ì‹œ íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.")
    st.stop()

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.success("âœ… ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ")

    okt = Okt()

    def extract_nouns(text):
        nouns = okt.nouns(text)
        return [n for n in nouns if n in preserve_keywords or (n not in stopwords and len(n) > 1)]

    df['tokens'] = df['title'].astype(str).apply(extract_nouns)
    texts = df['tokens'].tolist()
    titles = df['title'].tolist()

    st.subheader("ğŸ“Š ê°„ë‹¨í•œ ë¶„ì„ ê²°ê³¼")
    st.markdown(f"- ì „ì²´ ë¬¸ì„œ ìˆ˜: **{len(df)}**")
    total_words = sum(len(t) for t in texts)
    unique_words = len(set(word for t in texts for word in t))
    st.markdown(f"- ì¶”ì¶œëœ ì „ì²´ ë‹¨ì–´ ìˆ˜: **{total_words}**, ê³ ìœ  ë‹¨ì–´ ìˆ˜: **{unique_words}**")
    sample_keywords = list(set(word for t in texts for word in t))[:10]
    st.markdown(f"- ì˜ˆì‹œ ë‹¨ì–´: {', '.join(sample_keywords)}")

    report_text = f"""ì´ ë¬¸ì„œ ìˆ˜: {len(df)}
ì „ì²´ ë‹¨ì–´ ìˆ˜: {total_words}
ê³ ìœ  ë‹¨ì–´ ìˆ˜: {unique_words}
ì˜ˆì‹œ ë‹¨ì–´: {', '.join(sample_keywords)}\n"""

    st.subheader("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
    all_nouns = [noun for text in texts for noun in text]
    word_freq = Counter(all_nouns)
    report_text += f"""\nì›Œë“œí´ë¼ìš°ë“œ ì£¼ìš” ë‹¨ì–´:\n{word_freq.most_common(10)}\n"""
    wcloud = WordCloud(font_path="C:/Windows/Fonts/malgun.ttf", background_color='white', width=800, height=400)
    wcloud.generate_from_frequencies(word_freq)
    fig_wc = plt.figure(figsize=(10, 5))
    plt.imshow(wcloud, interpolation='bilinear')
    plt.axis('off')
    st.pyplot(fig_wc)

    if st.button("ğŸ§  GPTë¡œ ì›Œë“œí´ë¼ìš°ë“œ í•´ì„"):
        prompt = f"ë‹¤ìŒ ë‹¨ì–´ ë¹ˆë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì£¼ìš” ì´ìŠˆë‚˜ ì£¼ì œë¥¼ ìš”ì•½í•´ ì£¼ì„¸ìš”: {word_freq.most_common(20)}"
        response = openai.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        st.markdown(response.choices[0].message.content)

    st.subheader("ğŸ”— N-gram ë„¤íŠ¸ì›Œí¬")
    bigrams = list(combinations(all_nouns, 2))
    bigram_freq = Counter(bigrams)
    G = nx.Graph()
    for (a, b), freq in bigram_freq.items():
        if freq >= 3 and a != b:
            G.add_edge(a, b, weight=freq)

    pos = nx.spring_layout(G, k=0.6, seed=42)
    fig_ng, ax = plt.subplots(figsize=(12, 8))
    nx.draw_networkx_nodes(G, pos, node_color='skyblue', node_size=800)
    nx.draw_networkx_edges(G, pos, alpha=0.4)
    nx.draw_networkx_labels(G, pos, font_family='Malgun Gothic', font_size=10)
    st.pyplot(fig_ng)

    st.subheader("ğŸŒ ì¤‘ì‹¬ì„± ë¶„ì„")
    degree = nx.degree_centrality(G)
    betweenness = nx.betweenness_centrality(G)
    closeness = nx.closeness_centrality(G)
    eigenvector = nx.eigenvector_centrality(G)

    centrality_df = pd.DataFrame({
        "ë‹¨ì–´": list(degree.keys()),
        "Degree": [round(degree[n], 4) for n in degree],
        "Betweenness": [round(betweenness[n], 4) for n in degree],
        "Closeness": [round(closeness[n], 4) for n in degree],
        "Eigenvector": [round(eigenvector[n], 4) for n in degree]
    }).sort_values(by="Degree", ascending=False)

    st.dataframe(centrality_df.head(20))
    report_text += f"""\nì¤‘ì‹¬ì„± ìƒìœ„ ë‹¨ì–´:\n{centrality_df.head(10).to_string(index=False)}\n"""

    if st.button("ğŸ§  GPTë¡œ ì¤‘ì‹¬ì„± í•´ì„"):
        prompt = f"""ë‹¤ìŒì€ ë‹¨ì–´ ì¤‘ì‹¬ì„± ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ì¤‘ìš”í•œ í‚¤ì›Œë“œì™€ ì˜ë¯¸ë¥¼ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n{centrality_df.head(10).to_string(index=False)}"""
        response = openai.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        st.markdown(response.choices[0].message.content)

    st.subheader("ğŸ“š LDA í† í”½ ëª¨ë¸ë§")
    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    ldamodel = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=10, random_state=42)

    topic_keywords = {}
    report_text += """\nLDA í† í”½ í‚¤ì›Œë“œ:\n"""
    for idx, topic in ldamodel.show_topics(num_words=5, formatted=False):
        keywords = ", ".join([word for word, prob in topic])
        st.markdown(f"**í† í”½ {idx + 1}:** {keywords}")
        topic_keywords[f"Topic {idx + 1}"] = [word for word, _ in topic]
        report_text += f"Topic {idx + 1}: {keywords}\n"

    if st.button("ğŸ§  GPTë¡œ í† í”½ í•´ì„"):
        prompt = """ë‹¤ìŒì€ LDA í† í”½ ëª¨ë¸ë§ í‚¤ì›Œë“œì…ë‹ˆë‹¤. ê° í† í”½ì˜ ì£¼ì œë¥¼ ìš”ì•½í•˜ê³  ìœ ì¶” ê°€ëŠ¥í•œ ì´ìŠˆë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”:\n"""
        for k, v in topic_keywords.items():
            prompt += f"{k}: {', '.join(v)}\n"
        response = openai.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        st.markdown(response.choices[0].message.content)

    pdf = FPDF()
    pdf.add_page()
    font_path = "C:/Windows/Fonts/malgun.ttf"
    pdf.add_font('Malgun', '', font_path, uni=True)
    pdf.set_font("Malgun", '', 12)
    for line in report_text.split("\n"):
        pdf.cell(0, 10, txt=line, ln=True)

    pdf_path = "analysis_report.pdf"
    pdf.output(pdf_path)

    with open(pdf_path, "rb") as f:
        st.download_button("ğŸ“„ PDF ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ", f, file_name="analysis_report.pdf", mime="application/pdf")
